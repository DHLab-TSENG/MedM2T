{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: CVD - Unimodality (Pretrained Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root_path = ...\n",
    "\n",
    "from config import device, data_folder, log_folder\n",
    "import pickle\n",
    "task_dir = \"CVD\"\n",
    "data_folder+=task_dir+\"/\"\n",
    "log_folder+=task_dir+\"/\"\n",
    "\n",
    "from itertools import combinations,product\n",
    "from models.unimodal import create_unimodal_model\n",
    "from models.multimodal import create_multimodal_model\n",
    "from training_evaluation import run_kfolds\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.CVD.static import StaticLoader\n",
    "static_dataset_path = data_folder+\"/static.pkl\"\n",
    "if os.path.exists(static_dataset_path):\n",
    "    with open(static_dataset_path, \"rb\") as f:\n",
    "        static = pickle.load(f)\n",
    "else:\n",
    "    static = StaticLoader()\n",
    "    with open(static_dataset_path, \"wb\") as f:\n",
    "        pickle.dump(static, f)\n",
    "ids = static.get_ids()\n",
    "targets_df = static.get_targets()\n",
    "targets = targets_df.labels.values\n",
    "targets_dict = static.get_labels_dict()\n",
    "targets_num = len(targets_dict)\n",
    "subject_ids = targets_df.subject_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils.data_selection as stool\n",
    "import pickle\n",
    "kfolds_fpath = root_path+\"datasets/CVD/kfolds.pkl\"\n",
    "if os.path.exists(kfolds_fpath):\n",
    "    with open(kfolds_fpath, \"rb\") as f:\n",
    "        kfolds = pickle.load(f)\n",
    "else:\n",
    "    kfolds = stool.train_valid_test_kfolds_for_task1(ids, targets, subject_ids)\n",
    "    with open(kfolds_fpath, \"wb\") as f:\n",
    "        pickle.dump(kfolds, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.unimodal import create_unimodal_model\n",
    "from models.multimodal import create_multimodal_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_loss_weights(targets):\n",
    "    \"\"\"\n",
    "    Get the loss weights for the model.\n",
    "    \"\"\"\n",
    "    _targets, _num = np.unique(targets, return_counts=True)\n",
    "    loss_weights = _num/np.sum(_num)\n",
    "    loss_weights = [float(w) for w in loss_weights]\n",
    "    print(_targets, loss_weights)\n",
    "    return loss_weights\n",
    "\n",
    "LOSS_WEIGHTS = get_loss_weights(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "def data_normalization(data):\n",
    "    if type(data) is np.ndarray:\n",
    "        _mean = np.mean(data, axis=0)\n",
    "        _std = np.std(data, axis=0) + 1e-8\n",
    "    elif type(data) is torch.Tensor:\n",
    "        _mean = torch.mean(data, dim=0)\n",
    "        _std = torch.std(data, dim=0) + 1e-8\n",
    "    data = (data - _mean) / _std\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_type = \"extended\" # core or extended\n",
    "static_dataset = static.get_dataset(type=static_type)\n",
    "static_dataset_param = {\n",
    "    \"path\": static_dataset_path,\n",
    "    \"num\": len(static_dataset),\n",
    "    \"type\": static_type\n",
    "}\n",
    "if static_type == \"core\":\n",
    "    static_feats = static.feats_core\n",
    "else:\n",
    "    static_feats = static.feats_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blocks.mlp import  MLP, MLPDecoder\n",
    "\n",
    "FEATS_NUM = len(static_feats)\n",
    "EMBED_DIM = 128\n",
    "LR = 0.001\n",
    "ENCODER_DROPOUT = 0.1\n",
    "DECODER_DROPOUT = 0.5\n",
    "\n",
    "#MLP\n",
    "MLP_param = {\n",
    "    \"in_dim\": FEATS_NUM,\n",
    "    \"hidden_dim\": [EMBED_DIM, EMBED_DIM],\n",
    "    \"drop_prob\": ENCODER_DROPOUT\n",
    "}\n",
    "\n",
    "#Setting Decoder\n",
    "#MLPDecoder\n",
    "MLP_decoder_param = {\n",
    "    \"in_dim\": EMBED_DIM,\n",
    "    \"num_class\": targets_num,\n",
    "    \"hidden_dim\": [EMBED_DIM//2],\n",
    "    \"drop_prob\": DECODER_DROPOUT\n",
    "}\n",
    "\n",
    "#Setting Training Parameters\n",
    "train_param = {\n",
    "    \"DATASET\": static_dataset_param,\n",
    "    \"MODEL_NAME\": \"static_unimodal\",\n",
    "    \"ENCODER_PARAM\": [MLP_param],\n",
    "    \"ENCODER_MODEL\": [MLP.__name__],\n",
    "    \"DECODER_PARAM\": MLP_decoder_param,\n",
    "    \"DECODER_MODEL\": MLPDecoder.__name__,\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"LR\": LR,\n",
    "    \"MAX_EPOCHS\": 20,\n",
    "    \"OPTIMIZER\": \"Adam\"\n",
    "}\n",
    "\n",
    "model = create_unimodal_model(train_param[\"ENCODER_MODEL\"], \n",
    "                            train_param[\"ENCODER_PARAM\"], \n",
    "                            train_param[\"DECODER_MODEL\"], \n",
    "                            train_param[\"DECODER_PARAM\"], device)\n",
    "\n",
    "log = run_kfolds(train_param, model, static_dataset, kfolds, log_folder=log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.CVD.labs import LabsLoader\n",
    "labs_dataset_path = data_folder+\"/labs.pkl\"\n",
    "if os.path.exists(labs_dataset_path):\n",
    "    with open(labs_dataset_path, \"rb\") as f:\n",
    "        labs = pickle.load(f)\n",
    "else:\n",
    "    labs = LabsLoader(ids, targets)\n",
    "    with open(labs_dataset_path, \"wb\") as f:\n",
    "        pickle.dump(labs, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "labs_dataset = labs.get_dataset(only_valid=True)\n",
    "labs_ids = labs.get_ids(only_valid=True)\n",
    "\n",
    "labs_dataset_param = {\n",
    "    \"path\": labs_dataset_path,\n",
    "    \"only_valid\": True,\n",
    "    \"num\": len(labs_dataset)\n",
    "}\n",
    "\n",
    "#getting the labs kfolds(subsets of the kfolds, only with the valid labs)\n",
    "labs_kfolds = stool.get_sub_kfolds(ids, labs_ids, kfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blocks.embedding import TimeWinEmbedding\n",
    "from blocks.rnn import LSTM\n",
    "from blocks.mlp import  MLP, MLPDecoder\n",
    "from datasets.collate_fun import CreateCustomDataset, time_win_tokens_batch\n",
    "\n",
    "#get values/souces vocab numbers\n",
    "labs_val_vsize = labs.values_None_label+1\n",
    "labs_src_vsize = labs.sources_None_label+1 \n",
    "\n",
    "EMBED_DIM = 512\n",
    "LR = 7.5e-05\n",
    "ENCODER_DROPOUT = 0.05\n",
    "DECODER_DROPOUT = 0.1\n",
    "\n",
    "#Setting Encoder\n",
    "#TimeWinEmbedding\n",
    "TWEmbed_param = {\n",
    "    \"value_vocab_size\":labs_val_vsize, \n",
    "    \"source_vocab_size\":labs_src_vsize, \n",
    "    \"win_size\":labs.win_num, \n",
    "    \"embed_dim\":EMBED_DIM, \n",
    "    \"device\":device, \n",
    "    \"temporal_weighted\":False, \n",
    "    \"shared_embedding\":True\n",
    "}\n",
    "\n",
    "#BiLSTM\n",
    "LSTM_param = {\n",
    "    \"input_size\": EMBED_DIM,\n",
    "    \"hidden_size\": EMBED_DIM//2,\n",
    "    \"num_layers\": 2,\n",
    "    \"bidirectional\":True\n",
    "}\n",
    "\n",
    "#MLP\n",
    "MLP_param = {\n",
    "    \"in_dim\": EMBED_DIM,\n",
    "    \"hidden_dim\": [EMBED_DIM, EMBED_DIM],\n",
    "    \"drop_prob\": ENCODER_DROPOUT\n",
    "}\n",
    "\n",
    "#Setting Decoder\n",
    "#MLPDecoder\n",
    "MLP_decoder_param = {\n",
    "    \"in_dim\": EMBED_DIM,\n",
    "    \"num_class\": targets_num,\n",
    "    \"hidden_dim\": [EMBED_DIM//2],\n",
    "    \"drop_prob\": DECODER_DROPOUT\n",
    "}\n",
    "\n",
    "collate_fn_params = [\n",
    "    {\"name\":time_win_tokens_batch.__name__, \"param\":{\"accum\":True,\"onset\":False}},\n",
    "    {\"name\":time_win_tokens_batch.__name__, \"param\":{\"accum\":True,\"onset\":False}},\n",
    "]\n",
    "\n",
    "#Setting Training Parameters\n",
    "train_param = {\n",
    "    \"DATASET\": labs_dataset_param,\n",
    "    \"MODEL_NAME\": \"labs_unimodal\",\n",
    "    \"ENCODER_PARAM\": [TWEmbed_param, LSTM_param, MLP_param],\n",
    "    \"ENCODER_MODEL\": [TimeWinEmbedding.__name__, LSTM.__name__, MLP.__name__],\n",
    "    \"DECODER_PARAM\": MLP_decoder_param,\n",
    "    \"DECODER_MODEL\": MLPDecoder.__name__,\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"LR\": LR,\n",
    "    \"MAX_EPOCHS\": 20,\n",
    "    \"OPTIMIZER\": \"Adam\",\n",
    "    \"COLLATE_FN_PARAMS\": collate_fn_params\n",
    "}\n",
    "\n",
    "model = create_unimodal_model(train_param[\"ENCODER_MODEL\"], \n",
    "                            train_param[\"ENCODER_PARAM\"], \n",
    "                            train_param[\"DECODER_MODEL\"], \n",
    "                            train_param[\"DECODER_PARAM\"], device)\n",
    "\n",
    "collate_batch = CreateCustomDataset(len(collate_fn_params), train_param[\"COLLATE_FN_PARAMS\"])\n",
    "log = run_kfolds(train_param, model, labs_dataset, labs_kfolds, collate_fun=collate_batch, log_folder=log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.CVD.ecg import ECGLoader\n",
    "ecg_dataset_path = data_folder+\"/ecg.pkl\"\n",
    "if os.path.exists(ecg_dataset_path):\n",
    "    with open(ecg_dataset_path, \"rb\") as f:\n",
    "        ecg = pickle.load(f)\n",
    "else:\n",
    "    ecg = ECGLoader(ids, targets)\n",
    "    with open(ecg_dataset_path, \"wb\") as f:\n",
    "        pickle.dump(ecg, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "ecg_ids = ecg.get_ids(only_valid=True)\n",
    "ecg_kfolds = stool.get_sub_kfolds(ids, ecg_ids, kfolds)\n",
    "ecg_sig_kfolds = []\n",
    "for _train_idx, _valid_idx, _test_idx in ecg_kfolds:\n",
    "    ecg_sig_kfolds.append([ecg.get_recordi2ecgi(_train_idx, only_valid=True), \n",
    "                           ecg.get_recordi2ecgi(_valid_idx, only_valid=True),\n",
    "                           ecg.get_recordi2ecgi(_test_idx, only_valid=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_note_dataset = ecg.get_ecg_dataset(type=\"tokens\")\n",
    "ecg_note_dataset_param = {\n",
    "    \"path\": ecg_dataset_path,\n",
    "    \"num\": len(ecg_note_dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blocks.embedding import Embedding\n",
    "from blocks.mlp import MLP, MLPDecoder\n",
    "from datasets.collate_fun import CreateCustomDataset, tokens_batch\n",
    "ecg_vocab_size = int(ecg.ecg_statement_None_label+1)\n",
    "EMBED_DIM = 512\n",
    "LR = 0.0005\n",
    "\n",
    "Embedding_param = {\n",
    "    \"vocab_size\": ecg_vocab_size,\n",
    "    \"embed_size\": EMBED_DIM,\n",
    "}\n",
    "\n",
    "MLP_param = {\n",
    "    \"in_dim\": EMBED_DIM,\n",
    "    \"hidden_dim\": [EMBED_DIM, EMBED_DIM],\n",
    "    \"drop_prob\": 0.25\n",
    "}\n",
    "\n",
    "MLP_decoder_param = {\n",
    "    \"in_dim\": EMBED_DIM,\n",
    "    \"num_class\": targets_num,\n",
    "    \"hidden_dim\": [EMBED_DIM//2],\n",
    "    \"drop_prob\": 0.25\n",
    "}\n",
    "\n",
    "collate_fn_params = [{\"name\": tokens_batch.__name__, \"param\": {\"accum\": False, \"onset\": True}}]\n",
    "\n",
    "#Setting Training Parameters\n",
    "train_param = {\n",
    "    \"DATASET\": ecg_note_dataset_param,\n",
    "    \"MODEL_NAME\": \"ecg_note_unimodal\",\n",
    "    \"ENCODER_PARAM\": [Embedding_param, MLP_param],\n",
    "    \"ENCODER_MODEL\": [Embedding.__name__, MLP.__name__],\n",
    "    \"DECODER_PARAM\": MLP_decoder_param,\n",
    "    \"DECODER_MODEL\": MLPDecoder.__name__,\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"LR\": LR,\n",
    "    \"MAX_EPOCHS\": 20,\n",
    "    \"OPTIMIZER\": \"Adam\",\n",
    "    \"COLLATE_FN_PARAMS\": collate_fn_params\n",
    "}\n",
    "\n",
    "model = create_unimodal_model(train_param[\"ENCODER_MODEL\"],\n",
    "                            train_param[\"ENCODER_PARAM\"],\n",
    "                            train_param[\"DECODER_MODEL\"],\n",
    "                            train_param[\"DECODER_PARAM\"], device)\n",
    "\n",
    "collate_batch = CreateCustomDataset(len(collate_fn_params), train_param[\"COLLATE_FN_PARAMS\"])   \n",
    "log = run_kfolds(train_param, model, ecg_note_dataset, ecg_sig_kfolds, log_folder=log_folder, collate_fun=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_sig_dataset = ecg.get_ecg_dataset(type=\"sig\")\n",
    "ecg_sig_dataset_param = {\n",
    "    \"path\": ecg_dataset_path,\n",
    "    \"num\": len(ecg_sig_dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blocks.resnet import ResNet1d\n",
    "from blocks.mlp import MLP, MLPDecoder\n",
    "EMBED_DIM = 640\n",
    "LR = 0.0005\n",
    "ECG_CHANNEL = 12\n",
    "SIG_LEN = 640\n",
    "FILTER_SIZE = [64, 128, 196, 256, 320]\n",
    "SEQ_LEN = [640, 320, 160, 40, 20]\n",
    "\n",
    "ResNet_param = {\n",
    "    \"input_dim\": (ECG_CHANNEL, SIG_LEN),\n",
    "    \"blocks_dim\": list(zip(FILTER_SIZE, SEQ_LEN)),\n",
    "    \"kernel_size\": 5,\n",
    "    \"dropout_rate\": 0.3\n",
    "}\n",
    "\n",
    "MLP_param = {\n",
    "    \"in_dim\": FILTER_SIZE[-1] * SEQ_LEN[-1],\n",
    "    \"hidden_dim\": [EMBED_DIM],\n",
    "    \"drop_prob\": 0.1\n",
    "}\n",
    "\n",
    "MLP_decoder_param = {\n",
    "    \"in_dim\": MLP_param[\"hidden_dim\"][-1],\n",
    "    \"num_class\": targets_num,\n",
    "    \"hidden_dim\": [EMBED_DIM//2],\n",
    "    \"drop_prob\": 0.1\n",
    "}\n",
    "\n",
    "train_param = {\n",
    "    \"DATASET\": ecg_sig_dataset_param,\n",
    "    \"MODEL_NAME\": \"ecg_sig_unimodal\",\n",
    "    \"ENCODER_PARAM\": [ResNet_param, MLP_param],\n",
    "    \"ENCODER_MODEL\": [ResNet1d.__name__, MLP.__name__],\n",
    "    \"DECODER_PARAM\": MLP_decoder_param,\n",
    "    \"DECODER_MODEL\": MLPDecoder.__name__,\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"LR\": LR,\n",
    "    \"MAX_EPOCHS\": 1,\n",
    "    \"OPTIMIZER\": \"Adam\"\n",
    "}\n",
    "\n",
    "model = create_unimodal_model(train_param[\"ENCODER_MODEL\"],\n",
    "                                train_param[\"ENCODER_PARAM\"],\n",
    "                                train_param[\"DECODER_MODEL\"],\n",
    "                                train_param[\"DECODER_PARAM\"], device)\n",
    "\n",
    "log = run_kfolds(train_param, model, ecg_sig_dataset, ecg_sig_kfolds, log_folder=log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_feats_dataset = ecg.get_ecg_dataset(type=\"feats\")\n",
    "ecg_feats_dataset_param = {\n",
    "    \"path\": ecg_dataset_path,\n",
    "    \"num\": len(ecg_feats_dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blocks.mlp import MLP, MLPDecoder\n",
    "\n",
    "FEATS_NUM = ecg.ecg_feats.shape[1]\n",
    "EMBED_DIM = 64\n",
    "LR = 0.005\n",
    "\n",
    "MLP_param = {\n",
    "    \"in_dim\": FEATS_NUM,\n",
    "    \"hidden_dim\": [EMBED_DIM],\n",
    "    \"drop_prob\": 0.05\n",
    "}\n",
    "\n",
    "MLP_decoder_param = {\n",
    "    \"in_dim\": MLP_param[\"hidden_dim\"][-1],\n",
    "    \"num_class\": targets_num,\n",
    "    \"hidden_dim\": [EMBED_DIM//2],\n",
    "    \"drop_prob\": 0.05\n",
    "}\n",
    "\n",
    "train_param = {\n",
    "    \"DATASET\": ecg_feats_dataset_param,\n",
    "    \"MODEL_NAME\": \"ecg_feats_unimodal\",\n",
    "    \"ENCODER_PARAM\": [MLP_param],\n",
    "    \"ENCODER_MODEL\": [MLP.__name__],\n",
    "    \"DECODER_PARAM\": MLP_decoder_param,\n",
    "    \"DECODER_MODEL\": MLPDecoder.__name__,\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"LR\": LR,\n",
    "    \"MAX_EPOCHS\": 20,\n",
    "    \"OPTIMIZER\": \"Adam\"\n",
    "    # \"LOSS_WEIGHT\": [0.1,0.9]\n",
    "}\n",
    "\n",
    "model = create_unimodal_model(train_param[\"ENCODER_MODEL\"],\n",
    "                                train_param[\"ENCODER_PARAM\"],\n",
    "                                train_param[\"DECODER_MODEL\"],\n",
    "                                train_param[\"DECODER_PARAM\"], device)\n",
    "\n",
    "\n",
    "log = run_kfolds(train_param, model, ecg_feats_dataset, ecg_kfolds, log_folder=log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets.CVD.ecg import ECGFusionDataset\n",
    "\n",
    "ecg_fusion_dataset = ecg.get_ecg_dataset(type=\"fusion\")\n",
    "\n",
    "ecg_fusion_dataset_param = {\n",
    "    \"path\": ecg_dataset_path,\n",
    "    \"num\": len(ecg_fusion_dataset),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.multimodal import BiModalAttn\n",
    "from blocks.mlp import MLPDecoder, MLP\n",
    "from datasets.collate_fun import CreateCustomDataset, tokens_batch, basic_collate_fn\n",
    "import math\n",
    "\n",
    "k_models = True\n",
    "EMBED_DIM = 256\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.00005\n",
    "\n",
    "encoders_i = [0,1,2]\n",
    "\n",
    "ecg_sig_model_param = {\n",
    "    \"model_path\": log_folder+\"/ecg_sig_unimodal/.../\",\n",
    "    \"out_dim\": 640\n",
    "}\n",
    "\n",
    "ecg_feats_model_param = {\n",
    "    \"model_path\": log_folder+\"/ecg_feats_unimodal/.../\",\n",
    "    \"out_dim\": 512\n",
    "}\n",
    "\n",
    "ecg_note_model_param = {\n",
    "    \"model_path\": log_folder+\"/ecg_note_unimodal/.../\",\n",
    "    \"out_dim\": 256\n",
    "}\n",
    "\n",
    "BiModelAttn_param = {\n",
    "    \"embed_size\": EMBED_DIM,\n",
    "    \"num_blocks\": 1,\n",
    "    \"num_heads\": 64,\n",
    "    \"drop_prob\": 0.1,\n",
    "    \"fusion_type\": \"add\"\n",
    "}\n",
    "\n",
    "shared_layer_param = {\n",
    "    \"in_dim\": EMBED_DIM,\n",
    "    \"hidden_dim\": [EMBED_DIM],\n",
    "    \"drop_prob\": 0.05,\n",
    "    \"BatchNorm\": False\n",
    "}\n",
    "\n",
    "DECODER_IN_DIM = int(EMBED_DIM*len(encoders_i) + EMBED_DIM*math.comb(len(encoders_i), 2))\n",
    "\n",
    "MLP_decoder_param = {\n",
    "    \"in_dim\": DECODER_IN_DIM,\n",
    "    \"num_class\": targets_num,\n",
    "    \"hidden_dim\": [DECODER_IN_DIM//2],\n",
    "    \"drop_prob\": 0.1\n",
    "}\n",
    "\n",
    "collate_fn_params = [\n",
    "    {\"name\": basic_collate_fn.__name__},\n",
    "    {\"name\": basic_collate_fn.__name__},\n",
    "    {\"name\": tokens_batch.__name__, \"param\": {\"accum\": False, \"onset\": True}}\n",
    "]\n",
    "\n",
    "train_param = {\n",
    "    \"DATASET\": ecg_fusion_dataset_param,\n",
    "    \"MODEL_NAME\": \"ecg_fusion_unimodal\",\n",
    "    \"ENCODERS_I\": encoders_i,\n",
    "    \"ENCODERS_PARAM\": [ecg_sig_model_param, ecg_feats_model_param, ecg_note_model_param],\n",
    "    \"INTER_MODEL\": BiModalAttn.__name__,\n",
    "    \"INTER_MODEL_PARAM\": BiModelAttn_param,\n",
    "    \"SHARED_LAYER_PARAM\": shared_layer_param,\n",
    "    \"DECODER_MODEL\": MLPDecoder.__name__,\n",
    "    \"DECODER_PARAM\": MLP_decoder_param,\n",
    "    \"EMBED_DIM\": EMBED_DIM,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"LR\": LR,\n",
    "    \"MAX_EPOCHS\": 20,\n",
    "    \"OPTIMIZER\": \"Adam\",\n",
    "    \"COLLATE_FN_PARAMS\": collate_fn_params\n",
    "}\n",
    "\n",
    "model = create_multimodal_model(train_param, device, k_models=k_models)\n",
    "\n",
    "collate_batch = CreateCustomDataset(len(collate_fn_params), train_param[\"COLLATE_FN_PARAMS\"])\n",
    "log = run_kfolds(train_param, model, ecg_fusion_dataset, ecg_sig_kfolds, log_folder=log_folder, collate_fun = collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Time-Aware Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_dataset = ecg.get_dataset(only_valid=True)\n",
    "ecg_dataset_param = {\n",
    "    \"path\": ecg_dataset_path,\n",
    "    \"only_valid\": True,\n",
    "    \"num\": len(ecg_dataset)\n",
    "}\n",
    "del ecg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from datasets.collate_fun import CreateCustomDataset, ecg_bags_batch\n",
    "from blocks.mlp import MLP, MLPDecoder\n",
    "from models.unimodal import TemporalPooling\n",
    "from blocks.resnet import ResNet1d\n",
    "\n",
    "WIN_NUM = 5\n",
    "EMBED_DIM = 256\n",
    "LR = 0.00001\n",
    "BATCH_SIZE = 32\n",
    "k_models = 5\n",
    "\n",
    "TemporalPooling_param = {\n",
    "    \"model_path\": log_folder+\"/ecg_fusion_unimodal/250522013858/\",\n",
    "    \"win_size\": WIN_NUM,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "embeds_model = torch.load(TemporalPooling_param[\"model_path\"]+\"model_0.pth\", weights_only=False)\n",
    "for i in range(2):\n",
    "    embeds_model.shared_decoder.mlp[-(i+1)] = nn.Identity()\n",
    "embeds_dim = embeds_model.shared_decoder.mlp[-6].out_features\n",
    "TemporalPooling_param[\"embeds_model\"] = embeds_model\n",
    "\n",
    "ResNet_param = {\n",
    "    \"input_dim\": (embeds_dim, WIN_NUM),\n",
    "    \"blocks_dim\": [(embeds_dim//2, 5)],\n",
    "    \"kernel_size\": 3,\n",
    "    \"dropout_rate\": 0.3\n",
    "}\n",
    "\n",
    "MLP_param = {\n",
    "    \"in_dim\": ResNet_param[\"blocks_dim\"][-1][0] * ResNet_param[\"blocks_dim\"][-1][1],\n",
    "    \"hidden_dim\": [EMBED_DIM],\n",
    "    \"drop_prob\": 0.05\n",
    "}\n",
    "\n",
    "MLP_decoder_param = {\n",
    "    \"in_dim\": EMBED_DIM,\n",
    "    \"num_class\": targets_num,\n",
    "    \"hidden_dim\": [EMBED_DIM//2],\n",
    "    \"drop_prob\": 0.1\n",
    "}\n",
    "\n",
    "collate_fn_params = [{\"name\":ecg_bags_batch.__name__}]\n",
    "\n",
    "train_param = {\n",
    "    \"DATASET\": ecg_dataset_param,\n",
    "    \"MODEL_NAME\": \"ecg_unimodal\",\n",
    "    \"ENCODER_PARAM\": [TemporalPooling_param, ResNet_param, MLP_param],\n",
    "    \"ENCODER_MODEL\": [TemporalPooling.__name__, ResNet1d.__name__, MLP.__name__],\n",
    "    \"DECODER_PARAM\": MLP_decoder_param,\n",
    "    \"DECODER_MODEL\": MLPDecoder.__name__,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"LR\": LR,\n",
    "    \"MAX_EPOCHS\": 20,\n",
    "    \"OPTIMIZER\": \"Adam\",\n",
    "    \"COLLATE_FN_PARAMS\": collate_fn_params\n",
    "}\n",
    "\n",
    "model = []\n",
    "if k_models is not None:\n",
    "    for mi in range(k_models):\n",
    "        embeds_model = torch.load(TemporalPooling_param[\"model_path\"]+\"model_%i.pth\"%(mi), weights_only=False)\n",
    "        for i in range(2):\n",
    "            embeds_model.shared_decoder.mlp[-(i+1)] = nn.Identity()\n",
    "        TemporalPooling_param[\"embeds_model\"] = embeds_model\n",
    "        \n",
    "        _model = create_unimodal_model(train_param[\"ENCODER_MODEL\"],\n",
    "                                    train_param[\"ENCODER_PARAM\"],\n",
    "                                    train_param[\"DECODER_MODEL\"],\n",
    "                                    train_param[\"DECODER_PARAM\"], device)\n",
    "        model.append(_model)\n",
    "else:   \n",
    "    model = create_unimodal_model(train_param[\"ENCODER_MODEL\"],\n",
    "                                    train_param[\"ENCODER_PARAM\"],\n",
    "                                    train_param[\"DECODER_MODEL\"],\n",
    "                                    train_param[\"DECODER_PARAM\"], device)\n",
    "\n",
    "del TemporalPooling_param[\"embeds_model\"]\n",
    "\n",
    "collate_batch = CreateCustomDataset(1, train_param[\"COLLATE_FN_PARAMS\"])\n",
    "\n",
    "log = run_kfolds(train_param, model, ecg_dataset, ecg_kfolds, collate_fun=collate_batch, log_folder=log_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
